# Semantic Caching POC

##  Overview
This project implements a context aware semantic caching system for conversational AI. 

The goal is to reduce redundant LLM calls by reusing responses for semantically similar queries, like recognizing that some questions are essentially asking the same thing. Like for example:
- “What is the impact of climate change on corn yields?”    
- “How does global warming affect maize productivity?”  
 
This project uses Google Gemini embeddings and cosine similarity to identify and reuse relevant cached responses across conversations.

##  Personal Reflection
At first I wasn’t familiar with everything, but I tried breaking it down, researched, and figured it out piece by piece. I used AI tools to help me overcome errors such as latency calculation or any difficulty loading the API KEY and calling the LLM. 

This project was an opportunity to learn how embeddings, caching, and LLM work together in a real AI system. I first focused on getting the basic caching working, then progressively added embeddings, session tracking, and finally a similarity metric to improve performance with the consideration of context.

---

## Setup Instructions
### 1 Clone Repository
```bash
git clone https://github.com/<mbrit5>/semantic-cache-.git
cd semantic_caching_project
```
### 2 Install Dependencies
pip install -r requirements.txt

### 3 Environment Variables
Created a .env file with your Google Gemni API Key

### 4 Run Project
python main.py

## Design
When looking into the different types of cache designs I found semantic and hybrid designs. A semantic cache design was ideal since it consists of vector embeddings of query, and can still work even if the queries are phrase differently but still mean the same thing. Which is essentially the goal of the project. Whereas a Hybrid cache uses an exact string match and semantic similarity search. 
 

### Session Management
Each user has a session_id with a stored conversation history. This allows for the context to be carried onto multiple conversations.

### Context-Aware Embeddings
The cache key is generated by concatenating the conversation history, from user and AI, with the current query.

## Similarity Metric Choice
Before deciding on a similarity metric, I explored different options, including cosine similarity, euclidean distance, and dot product. Cosine similarity is widely used in natural language processing because it measures the angle between vectors, focusing on their direction rather than their magnitude. Euclidean distance measures absolute distance, but can be less reliable in high-dimensional spaces, while dot product is efficient but sensitive to vector length. Which makes cosine similarity well-suited for text embeddings, where the semantic meaning is captured in the orientation of the vector. The project goals was to focus on accuracy over speed, therefore, cosine similarity provides the best balance of interpretability, reliability, and ease of implentation.

## Vector Database Choice : Chroma DB
I had to look into the different vector database options, such as chroma DB, Faiss, Pinecone, or Qdrant. I ended up choosing Chroma DB because it would be able to easily store the session_id and raw LLM responses, it also provides an embedded vector database solution that is ideal for a POC. This is really helpful because the system was able to easily retrieve the cached response after performing the similarity search on the embedding. Additionally, the setup was simple and allowed for quick development and testing. As mentioned, Faiss was also considered, but it would require a separate dictionary to link the vectors to the responses. As for Pinecone or Qdrant, it was a more complex, and from my understanding, for much larger project. Which would be interesting to look into in the future. 

## Metrics
* **Latency (ms)** - Response time per query
* **Cache Hit Rate (%)** - Percentage of queries served from cache
* **LLM Calls Avoided** - Number of API calls saved

=== Cache Metrics ===
Total Calls: 4
Cache Hits: 2
Cache Misses: 2
LLM Calls Made: 2
Cache Hit Rate: 50.00%
Average Latency: 5585.94 ms
======================
### What does this mean?
The Cache metrics indicate how the average latency is around 5.5 seconds. It also shows the cache reduced the reduntant API calls, which means it improved the systems performance. 

## Results
### Test 1 (Miss)
Query Text: "What is the impact of climate change on corn yields?"
Similarity Score: N/A
Cache: Miss
Latency: 16.87 s

### Test 2 (Hit)
Query Text: "How does global warming affect maize productivity?"
Similarity Score: 0.8170	
Cache: Hit
Latency 0.22 s

### Test 3 (Hit)
Query Text: "What were the average yields in Iowa last year?"
Similarity Score: 0.8303	
Cache: Hit
Latency 0.29 s

### Test 3 (Miss)
Query Text: "What is the primary export from the American Midwest?"
Similarity Score: 0.5330		
Cache: Miss
Latency 4.96 s


## Trade-offs
### Threshold 
When deciding which threshold to choose, I focused on a range of 0.75 to 0.9, where I thought 0.75 to be too broad and possibly risk false positives, and 0.9 risked being too strict. I initially tested with 0.80, but my test run showed a similarity score of 0.7876 between the two queries ("corn yields" and "maize productivity"). Since 0.7876 < 0.80, the system registered a miss. It appeared to be too strict; therefore, I changed it to 0.75, which appeared to be more successful, as there was a hit. 

### Scalability - What breaks at millions of queries?
POC is limited because of Python's in-memory structure:
- My implementation stores all vectors in a simple python list and performs a linear search. So searching high dimension vectors is very difficult.
- Would have to change the database where the vectors are stored in, such as previously mentioned Pinecone or Qdrant.

### Eviction - What to remove from cache?
My code implements a Least Recently Used (LRU) policy to manage the cache size (MAX_CACHE_SIZE=3). When the limit is hit, the entry that has gone the longest without being accessed is removed.


## Conclusion

## Bonus
### 1. 


### 2. Part A


### 2. Part B
